{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# reveal.js presentation configuration\n",
    "from notebook.services.config import ConfigManager\n",
    "\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'league',\n",
    "              'transition': 'fade',\n",
    "              'center': 'false',\n",
    "              'overview' : 'true',\n",
    "              'start_slideshow_at': 'selected'\n",
    "})\n",
    "\n",
    "# imports\n",
    "import theano\n",
    "from theano import tensor\n",
    "import codecs\n",
    "import numpy\n",
    "import sys\n",
    "from blocks import initialization\n",
    "from blocks import roles\n",
    "from blocks.model import Model\n",
    "from blocks.bricks import Linear, NDimensionalSoftmax\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.serialization import load_parameters\n",
    "from blocks.bricks import NDimensionalSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language modeling with RNN\n",
    "\n",
    "[Fabio A. González](http://dis.unal.edu.co/~fgonza/), Universidad Nacional de Colombia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "* Training data: Biblia Reina Valera 1960\n",
    "* Software:\n",
    "  * [Blocks](https://github.com/mila-udem/blocks): \"Blocks is a framework that helps you build neural network models on top of Theano\"\n",
    "  * [Theano](http://deeplearning.net/software/theano/): \"Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 978848\n",
      "Vocabulary size: 85\n"
     ]
    }
   ],
   "source": [
    "# Load training file to get vocabulary\n",
    "text_file = 'biblia.txt' # input file\n",
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print \"Total number of chars:\", len(data)\n",
    "print \"Vocabulary size:\", vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s será medido. 3 ¿Y por qué miras la paja que está en el ojo de tu hermano, y no echas de ver la viga que está en tu propio ojo? 4 ¿O cómo dirás a tu hermano: Déjame sacar la paja de tu ojo, y he aquí la viga en el ojo tuyo? 5 ¡Hipócrita! saca primero la viga de tu propio ojo, y entonces verás bien para sacar la paja del ojo de tu hermano.\r\n",
      "\r\n",
      "6 No deis lo santo a los perros, ni echéis vuestras perlas delante de los cerdos, no sea que las pisoteen, y se vuelvan y os despedacen.\r\n",
      "\r\n",
      "La oración, y la regla de oro\r\n",
      "\r\n",
      "(Lc. 11.9-13; 6.31)\r\n",
      "\r\n",
      "7 Pedid, y se os dará; buscad, y hallaréis; llamad, y se os abrirá. 8 Porque todo aquel que pide, recibe; y el que busca, halla; y al que llama, se le abrirá. 9 ¿Qué hombre hay de vosotros, que si su hijo le pide pan, le dará una piedra? 10 ¿O si le pide un pescado, le dará una serpiente? 11 Pues si vosotros, siendo malos, sabéis dar buenas dádivas a vuestros hijos, ¿cuánto más vuestro Padre que está en los cielos dará buenas cosas a los que le pidan? 12 \n"
     ]
    }
   ],
   "source": [
    "print data[21000:22000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network architecture\n",
    "\n",
    "<img src=\"rnn_architecture.jpg\" width= 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model structure\n",
    "embedding_size = 256 # number of hidden units per layer\n",
    "\n",
    "# Input\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)\n",
    "\n",
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork1.name = 'fork1'\n",
    "grnn1 = GatedRecurrent(dim=embedding_size)\n",
    "grnn1.name = 'grnn1'\n",
    "\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'],\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "fork2.name = 'fork2'\n",
    "grnn2 = GatedRecurrent(dim=embedding_size)\n",
    "grnn2.name = 'grnn2'\n",
    "\n",
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Propagate x until top brick to get y_hat predictions\n",
    "x = tensor.imatrix('features')  # input\n",
    "y = tensor.imatrix('targets')   # output\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "h1.name = 'h1'\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "h2.name = 'h2'\n",
    "linear3 = hidden_to_output.apply(h2)\n",
    "linear3.name = 'linear3'\n",
    "y_hat = softmax.apply(linear3, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'\n",
    "\n",
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear3, extra_ndim=1).mean()\n",
    "cost.name = 'cost'\n",
    "\n",
    "model = Model(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load parameters and build Theano graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load model parameters from a file\n",
    "with open('grnn_best.tar') as model_file:\n",
    "    model_params = model.get_parameter_dict().keys()\n",
    "    param_vals = {k:v for k,v in load_parameters(model_file).iteritems() if k in model_params}\n",
    "    model.set_parameter_values(param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at theano_graph.svg\n"
     ]
    }
   ],
   "source": [
    "# Define Theano graph\n",
    "y, x = model.inputs\n",
    "softmax = NDimensionalSoftmax()\n",
    "linear_output = [v for v in model.variables if v.name == 'linear3'][0]\n",
    "y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
    "predict = theano.function([x], y_hat)\n",
    "#theano.printing.pydotprint(predict, outfile=\"theano_graph.svg\", format = 'svg', var_with_name_simple=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"theano_graph.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#take activations of last element\n",
    "activations = [h1[-1].flatten(), h2[-1].flatten()]\n",
    "initial_states = [grnn1.parameters[-1], grnn2.parameters[-1]]\n",
    "states_as_params = [tensor.vector(dtype=initial.dtype) for initial in initial_states]\n",
    "\n",
    "#Get prob. distribution of the last element in the last seq of the batch\n",
    "fprop = theano.function([x] + states_as_params, activations + [y_hat[-1, -1, :]], givens=zip(initial_states, states_as_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def sample(x_curr, states_values, fprop, temperature=1.0):\n",
    "    '''\n",
    "    Propagate x_curr sequence and sample next element according to\n",
    "    temperature sampling.\n",
    "    Return: sampled element and a list of the hidden activations produced by fprop.\n",
    "    '''\n",
    "    activations = fprop(x_curr, *states_values)\n",
    "    probs = activations.pop().astype('float64')\n",
    "    probs = probs / probs.sum()\n",
    "    if numpy.random.binomial(1, temperature) == 1:\n",
    "        sample = numpy.random.multinomial(1, probs).nonzero()[0][0]\n",
    "    else:\n",
    "        sample = probs.argmax()\n",
    "\n",
    "    return sample, activations, probs[sample]\n",
    "\n",
    "def init_params(primetext=u''):\n",
    "    if not primetext or len(primetext) == 0:\n",
    "        primetext = ix_to_char[numpy.random.randint(vocab_size)]\n",
    "    primetext = ''.join([ch for ch in primetext if ch in char_to_ix.keys()])\n",
    "    if len(primetext) == 0:\n",
    "        raise Exception('primetext characters are not in the vocabulary')\n",
    "    x_curr = numpy.expand_dims(\n",
    "        numpy.array([char_to_ix[ch] for ch in primetext], dtype='uint8'), axis=1)\n",
    "\n",
    "    states_values = [initial.get_value() for initial in initial_states]\n",
    "    return x_curr, states_values\n",
    "    \n",
    "def stochastic_sampling(length, primetext=u'', temperature=1.0):\n",
    "    x_curr, states_values = init_params(primetext)\n",
    "    sys.stdout.write('Starting sampling\\n' + primetext)\n",
    "    for _ in range(length):\n",
    "        idx, states_values, probs = sample(x_curr, states_values, fprop, temperature)\n",
    "        sys.stdout.write(ix_to_char[idx])\n",
    "        x_curr = [[idx]]\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "def beam_sampling(length, primetext=u'', beam_size=5, temperature=1.0):\n",
    "    x_curr, states_values = init_params(primetext)\n",
    "    inputs = [x_curr] * beam_size\n",
    "    states = [states_values] * beam_size\n",
    "    logprobs = numpy.zeros((beam_size, 1))\n",
    "    seqs = numpy.zeros((length+x_curr.shape[0], beam_size))\n",
    "    seqs[0:x_curr.shape[0], :] = numpy.repeat(x_curr, beam_size, axis=1)\n",
    "    for k in range(length):\n",
    "        probs = numpy.zeros((beam_size,beam_size))\n",
    "        indices = numpy.zeros((beam_size,beam_size), dtype='int32')\n",
    "        hstates = numpy.empty((beam_size,beam_size), dtype=list)\n",
    "        for i in range(beam_size):\n",
    "            for j in range(beam_size):\n",
    "                indices[i][j], hstates[i][j], probs[i][j] = sample(inputs[i], states[i], fprop, temperature)\n",
    "        probs = numpy.log(probs) + logprobs\n",
    "        best_idx = probs.argmax(axis=1)\n",
    "        inputs = [[[idx]] for idx in indices[range(beam_size), best_idx]]\n",
    "        states = [hs for hs in hstates[range(beam_size), best_idx]]\n",
    "        logprobs = probs[range(beam_size), best_idx].reshape((beam_size, 1))\n",
    "        seqs[k +x_curr.shape[0], :] = numpy.array(inputs).flatten()\n",
    "\n",
    "    return logprobs.flatten(), numpy.array(seqs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(s) = -51.402. Sample: blanco de los demonios, y le dijeron: ¿Quién es el que había de entre los hombres, de la ciudad de la multi\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -58.557. Sample: blanco de ellos, y le dijo: ¿Qué había entregado de la palabra de Dios y de la promesa de la carne, y le di\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -59.374. Sample: blanco de los demonios, y le dijo: ¿Qué había de la multitud, y no se le dijo: No te dijeron: ¿Quién es el \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -59.669. Sample: blanco para que no habían sido destruidos de la carne, y le dijo: Señor, ¿qué dijo: ¿Quién es el día de est\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -60.460. Sample: blanco de los demonios, y le dijo: ¿Qué que había de la ciudad, y le dijo: ¿Qué había de la carne, y le dij\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -60.686. Sample: blanco de Dios y los demonios, y le dijo: ¿Qué había de esta generación. 10 Porque el que de la ciudad de D\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "log P(s) = -63.916. Sample: blanco de los hombres, y no se levantarán a tu propia vez de la carne, y le dijo: Señor, ¿qué hará con voso\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "logprobs, seqs = beam_sampling(100, primetext=u'blanco ', beam_size = 7, temperature = 1.0)\n",
    "for i in logprobs.flatten().argsort()[::-1]:\n",
    "    print 'log P(s) = {0:3.3f}. Sample: '.format(logprobs.flatten()[i]) + u''.join([ix_to_char[ix] for ix in numpy.array(seqs).squeeze()[:,i]])\n",
    "    print '~' * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling from the model\n",
    "\n",
    "* The model calculates the probability of the next word given the previous words:  \n",
    "$$P(w_t | w_{t-1}, w_{t-2},\\dots, w_{1})$$\n",
    "* We sample from the model using this conditional probability\n",
    "  ```python\n",
    "  for i in [1..n]:\n",
    "      P = predict_next() \n",
    "      bin_var = sample_binomial(temperature)\n",
    "      if bin_var:\n",
    "          w_i = sample_multinomial(P) \n",
    "      else:\n",
    "          w_i = P.argmax() \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "El sentido de la vida esté confianzas de la carones de la Escritura de los doce de antes de los hombres, para los que están testros para que no te dijeron: ¿Quién le había de la carne, y amará al puel, por la fe de la multitud de Jesucristo, y le dijo: ¿Quién cree por las madre del cielo, también la propititad al que había de la carne, y vinieron al que había desada de los profetas, y ó la palabra de Jesucristo. 13 Por lo cual le dio a las cuales nos mandamientos de la casan, y los demonios de la carne, y le dijo: Padre, y le enseñaba de la carne, 4 para que no se maravillas a los cuales hablaban de carne, y le dijo: ¿Es lícito de la carne, y le dijeron: ¿Quién yo te digas queje de los que vayan a los que están de la ley, la cena de los cielos y los de la concie\n",
      "\n",
      "4 TIMOTEO 1\n",
      "\n",
      "1 Entonces el que había de los escribas y los demonios. 15 Todo el que me enciano, para que no les había de la conciencia de la ciudad, pues, que este testimonio de Jesucristo, que al que había de aquí, y le dijo: ¿Saego a Epístola del cielo, y le dijo: ¿Qué hace en el mar, 6 y le dijo: De cierto os dijeron: ¿Como está en los cielos.\n",
      "\n",
      "La predicación de la cual está escrito: \n",
      "\n",
      "\n",
      "He aquí, yo solamiento de la multitud, y de cuando ya había de la esperanza de la multitud, y vinieron a la multitud, y les dijo: ¿Qué habío de perdonado en el camino, y le dijo: ¿Había contenciones en la conciencia, y por la fe de la muerta, y la palabra que está en la multitud, y le dijo: Padre que es para hacer ni por las manos de la carne. 25 Porque el que había de la navegado, y temas de la carne? 15 Porque el Señor a él en el día de la carne, la cual está escrito: ¿Qué hace en pie, y había de la carne; porque el otro de la ciudad de la multitud de la carrera que recibirá al pueblo, y le dijo: ¿Quién es el que hablaba nada de los demoniados los hombres, y ley de los demonios, y le dijo: ¿Qué había de la carne, y le enviaron a la multitud, y le dijo: ¿Qué salvará al que había de reposo, y tendrá en la circuncisión, y le después de estas cosas, de los judíos de los que estén en dónde estaba en el día de los discípulos, y le aparedor de la ley, y que no se le dijo Jesús, y le dijo: ¿Quí, pues, se dijo: ¿Quién es el Señor Jesucris? ¿No está escrito: Lo que no se había de la carne, y le dijo: ¿Qué presenta, vino a Pedro, en Señor; pero si desde el mundo obedecen acabarlo de los demonios. 15 Porque el que a mí me ha de entre los hombres, y hablaban desde entre los santos en el camino, y glorificado en el cuerpo de los demonios, y le dijo: Señal de la venida de la gracia de Dios que había recibíra y a la madre, y que no está en la carne, en el hermano de los demonios, y le después de gran manera que no puede el viento de los que habían sido entre lugar. 18 Al oír este está en el considera, a los que estaban en aquellos días de la cera, sino que está en la ciudad, para que nos recibirá al pueblo. 17 Y vino a sus discípulos, y le aparían de los judíos, de otra vez en el día de la cárcel, 30 y le dijo: ¿Qué, pues, que est\n"
     ]
    }
   ],
   "source": [
    "stochastic_sampling(3000, primetext=u'El sentido de la vida es', temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability of a text\n",
    "* The probability of a text is:  \n",
    "$$P(w_1, \\dots, w_n) = P(w_1)\\prod_{i=2}^{n}\\ P(w_i | w_{1},\\dots, w_{i-1})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the probability of a text\n",
    "def log_likelihood(text):\n",
    "    text = ''.join([ch for ch in text if ch in char_to_ix])\n",
    "    x_curr = numpy.expand_dims(numpy.array([char_to_ix[ch] for ch in text], dtype='uint8'), axis=1)\n",
    "    probs = predict(x_curr).squeeze()\n",
    "    return sum([numpy.log(probs[i,c]) for i,c in enumerate(x_curr[1:].flatten())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-35.292520381161012"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(\"buscad, y hallaréis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.525116831064224"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(\"this is a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Most likely phrases from a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7872946688 el hombre ama a  \n",
      "30.1335569599   el hombre ama a\n",
      "30.9096414867   ama el hombre a\n",
      "31.6734288311 a el hombre ama  \n",
      "31.9067574043   a el hombre ama\n",
      "32.2993101903   a hombre el ama\n",
      "33.9996441508   el ama a hombre\n",
      "34.0615620876   ama a el hombre\n",
      "34.3336700925   el hombre a ama\n",
      "34.639605933 hombre el ama a  \n",
      "34.6567749001   hombre el ama a\n",
      "34.7339455399   ama a hombre el\n",
      "34.8366844942 el ama a hombre  \n",
      "35.4865090914   hombre ama el a\n",
      "35.7297849205   a hombre ama el\n",
      "35.8706177044 ama a el hombre  \n",
      "35.9484713499   hombre ama a el\n",
      "36.1151354631 a ama el hombre  \n",
      "36.5683493641 a hombre el ama  \n",
      "36.7281361914 ama el hombre a  \n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "bow =  [' ', 'hombre', 'ama', 'a', 'el']\n",
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:20]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least likely phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1275147402 ama hombre el   a\n",
      "52.1970617577 hombre a   el ama\n",
      "52.2630596872 el a   hombre ama\n",
      "52.3661701797 el   ama hombre a\n",
      "52.7321290059 el   hombre a ama\n",
      "52.8102573471 ama hombre   a el\n",
      "52.9034054916 ama a el   hombre\n",
      "52.9764832641 ama   el a hombre\n",
      "53.0416675795 ama a hombre   el\n",
      "53.2206440819 a ama el   hombre\n",
      "53.9376723702 hombre el a   ama\n",
      "54.0484542205 hombre   el a ama\n",
      "54.9569975241 hombre a ama   el\n",
      "55.3035467451 ama hombre   el a\n",
      "55.6084476279 el   a ama hombre\n",
      "56.4346062609 el a   ama hombre\n",
      "56.8539055093 el a ama   hombre\n",
      "57.2385296579 ama el a   hombre\n",
      "57.3769435149 a ama hombre   el\n",
      "57.7220643626 ama hombre a   el\n"
     ]
    }
   ],
   "source": [
    "perms = [' '.join(perm) for perm in permutations(bow)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-20:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.149269104 nump\n",
      "9.33271074295 mpun\n",
      "12.42851758 umpn\n",
      "13.1477912068 nmup\n",
      "13.6903936863 mnpu\n",
      "------------------\n",
      "22.3517570496 nupm\n",
      "27.4689741135 upnm\n",
      "29.4543595314 pnmu\n",
      "29.7688331604 upmn\n",
      "29.8283925056 pmnu\n"
     ]
    }
   ],
   "source": [
    "text = list(u'mnpu')\n",
    "perms = [''.join(perm) for perm in permutations(text)]\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[:5]:\n",
    "    print p, t\n",
    "print \"------------------\"\n",
    "for p, t in sorted([(-log_likelihood(text),text) for text in perms])[-5:]:\n",
    "    print p, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling\n",
      "(Lc. 11.1-1-20)\n",
      "\n",
      "13 Entonces le dijo: ¿Qué había de la carne, y le después de los que están en luz de la carne, y le dijo: ¿Qué había de la carne, y le dijo: Yo se levantarán a la multitud, y le dijo: ¿Qué había de la carne, y le dijo a los que están estas cosas, le dijeron: ¿Quién está en la conciencia, y le enviaron a la multitud de la carne, y le dijo: ¿Qué había de aquí, y le dijo: ¿Quién es el q\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print stochastic_sampling(400, u\"(Lc. \", temperature = 0.1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
